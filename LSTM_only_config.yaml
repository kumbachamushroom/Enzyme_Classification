data:
  train:
    num_samples: 550000# number of training samples, if -1 all samples are loaded
    path: '/home/lucas/PycharmProjects/Enzyme_Classification/csv_files/train_70.csv' # path to training data
    batch_size: 256 # batch size for training
  eval:
    num_samples: 200000 # number of eval samples, if -1 all samples are loaded
    path: '/home/lucas/PycharmProjects/Enzyme_Classification/csv_files/eval_20.csv' # path to eval data
    batch_size: 256 #batch size for eval
  test:
    num_samples: -1 # number of test samples, if -1 all samples are loaded
    path: '/home/lucas/PycharmProjects/Enzyme_Classification/csv_files/test_10.csv' # path to test data


model:
  n_epochs : 300 # number of epochs to train
  hidden_size: 256
  output_size: 20
  input_size: 1234
  wandb : False # True if you want to use wandb logger
  n_layers: 2 # number of hidden layers
  save_path: # path to save models to
  bi: False # True if you want bidirectional recurrent layer
  #recurrent_layer: 'gru' #Choose between 'gru' and 'lstm'
  lr_scheduler:
    initial_lr: 0.0001
    factor: 0.5
    patience: 5
    cooldown: 10
    min_lr: 0.00001
  weight_decay: 1e-7 #magnitude of weight decay
